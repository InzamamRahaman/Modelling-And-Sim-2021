{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inzamam/anaconda3/lib/python3.8/site-packages/IPython/core/magics/pylab.py:159: UserWarning: pylab import has clobbered these variables: ['annotate', 'ylim', 'arrow', 'xlim']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "from scipy.stats import kstest\n",
    "from plotnine import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Once of the chief concerns of early statistical computing was the generation of random numbers. Given that computers are, mostly, deterministic devices, generating true randomness appeared to be a pipedream not persuing. Consequently, the scientific community re-purposed their efforts towards pseudo-random number generation. In pseudo-random number generation, we use a deterministic proceedure to generate sequences of numbers that from a statistical point of view *appear* random. These pseudo-random number generation techniques proved suitable for the computational tasks requring randomness. There are many different techniques in this space running the gamut of the simple to the complex. Mersene Twistter is the most commonly used method in most modern applications. However, it is an incredibly complicated algorithm. The core concepts that undergird Mersene Twister also serve as the basis of older techniques that are much easier to understand. \n",
    "\n",
    "Currently, techniques that exploit sensor noise and environmental noise to generate true random numbers. However, the overhead of such techniques are too high to justify their use given that pseudo-random number generation can work just fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Congruential Generator\n",
    "\n",
    "The linear congruential generator is one of the most simple and easy to implement methods for pseudo-random number generation. An instance of LCG requires three paramaters:\n",
    "\n",
    "- $a$, a mutliplier\n",
    "- $c$, an increment\n",
    "- $m$, the max value\n",
    "\n",
    "Moreover, every instance of LCG also requires a seed value between 0 and $m$ inclusive, denoted as $X_0$; $X_0$ is often called the random seed. Each element is generated using the previous element as well as the aforementioned parameters. Each element is an integer between 0 and $m$ inclusive, and the overall sequence *appears* statistically indistinguisable from the discrete uniform distribution with limits 0 and $m$. Each element in the sequence can then be divided by $m$ to yield a floating point number between $0$ and $1$ to simulate the standard continuous uniform distribution. As we shall see later, the ability to simulate the standard continuous uniform distribution is a sort of master key that enables us to simulate any other arbitrary distribution.\n",
    "\n",
    "The core iterative process of the LCG method is as follows:\n",
    "\n",
    "$$X_{i + 1}= (aX_{i} + c) \\operatorname{mod} m$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LCG(object):\n",
    "    def __init__(self, a, c, m, X_0):\n",
    "        self.X_0 = X_0\n",
    "        self.a = a \n",
    "        self.c = c \n",
    "        self.m = m \n",
    "        self.curr = X_0\n",
    "    \n",
    "    def __call__(self):\n",
    "        self.curr = (self.a * self.curr + self.c) % self.m \n",
    "        return self.curr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcg = LCG(4, 3, 100, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "\n",
    "# generate 100 \"samples\" from our generator\n",
    "# divide by m to scale between 0 and 1\n",
    "arr = np.array([lcg()/100 for i in range(n)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Uniformity\n",
    "\n",
    "After generating our samples, we can evaluate the hypothesis that the generated sequence follows a standard uniform distribution. We can use the Kolmogorov-Smirnov test for goodness of fit. Recall that the hypothesis setup of the KS test is as follows:\n",
    "\n",
    "$$\n",
    "H_{0} \\text{: the distributions are the same}\n",
    "$$\n",
    "\n",
    "$$\n",
    "H_{A} \\text{: the distributions are different}\n",
    "$$\n",
    "\n",
    "If the KS test reports a *high* p-value, then we fail to reject the null hypothesis, i.e. $H_{0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KstestResult(statistic=0.09000000000000008, pvalue=0.37063202558632946)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kstest(arr, 'uniform')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the above p-value is greater than 0.1, we fail to reject $H_{0}$ and we take the distributions as being the same"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
